
# PrÃ¡ctica: Tu Propio ChatGPT Local con Docker ğŸ³ + IA ğŸ¤–

<img src="image.png" width="600px"></img>
## ğŸ¯ Objetivo

En esta prÃ¡ctica vamos a desmitificar la Inteligencia Artificial. Dejaremos de verla como una "magia" que ocurre en servidores de empresas externas y la convertiremos en una pieza mÃ¡s de nuestro stack tecnolÃ³gico.

**Vas a desplegar una arquitectura Full Stack de IA:**

1. **Backend:** Un servidor de inferencia (**Ollama**) corriendo en un contenedor Docker.
2. **Modelo:** Una LLM (Large Language Model) real (`TinyLlama`) descargada en tu mÃ¡quina.
3. **Frontend:** Una interfaz web (chat) que se comunicarÃ¡ con tu backend mediante una API REST.

---

## ğŸ› ï¸ Requisitos previos
* Tener instalado **Docker Desktop** (o Docker Engine).
* Visual Studio Code con la extensiÃ³n **Live Server**.
* Conocimientos bÃ¡sicos de Git (clonar, commit, push).

---

## ğŸš€ Pasos de la prÃ¡ctica

### 1. PreparaciÃ³n del Entorno

Clona este repositorio en tu mÃ¡quina local. EncontrarÃ¡s un fichero `index.html` que contiene la interfaz del chat (Frontend). **No es necesario modificar el HTML ni el JS**, pero Ã©chale un vistazo para entender cÃ³mo hace las llamadas `fetch`.

### 2. Infraestructura (Docker Compose)

Tu tarea principal es levantar el servicio de IA. Para ello, **debes crear un archivo llamado `docker-compose.yml**` en la raÃ­z del proyecto.

Copia y pega la siguiente configuraciÃ³n. FÃ­jate bien en la variable de entorno `OLLAMA_ORIGINS`, es vital para que tu navegador (Frontend) tenga permiso para hablar con el servidor (Backend) sin errores de CORS.

```yaml
version: '3.8'

services:
  ollama:
    image: ollama/ollama:latest
    container_name: mi_ia_local
    ports:
      - "11434:11434" # Puerto estÃ¡ndar de la API de Ollama
    volumes:
      - ./ollama_data:/root/.ollama # Persistencia: guarda el modelo para no descargarlo siempre
    environment:
      - OLLAMA_ORIGINS=* # Â¡CRÃTICO! Permite peticiones desde la web (CORS)
    restart: always

```

Una vez creado, levanta el servicio desde la terminal:

```bash
docker-compose up -d

```

### 3. InstalaciÃ³n del "Cerebro" (El Modelo)

Ahora mismo tienes el servidor (Ollama) corriendo, pero estÃ¡ "vacÃ­o". Necesitamos descargar un modelo de lenguaje. Usaremos **TinyLlama** porque es muy ligero y rÃ¡pido.

1. Entra en la terminal del contenedor:
```bash
docker exec -it mi_ia_local /bin/bash

```


2. Descarga y ejecuta el modelo:
```bash
ollama run tinyllama

```


*(VerÃ¡s una barra de progreso. Cuando termine, aparecerÃ¡ un prompt `>>>`. Puedes probar a escribir "Hola" para ver si responde. Para salir del chat escribe `/bye` y para salir del contenedor escribe `exit`).*

### 4. Conectando el Frontend1. Abre el archivo `index.html` en VS Code.
2. Haz clic derecho sobre el cÃ³digo y selecciona **"Open with Live Server"**.
3. Se abrirÃ¡ tu navegador. Escribe algo en el chat (ej: "Â¿QuÃ© es Java?") y espera la respuesta.
* *Nota:* La primera vez puede tardar unos segundos en "cargar" el modelo en la memoria RAM.



---

## ğŸ•µï¸ Fase de InvestigaciÃ³n y ExploraciÃ³n

Una vez que tengas el chat funcionando, dedica 20 minutos a investigar. La IA avanza muy rÃ¡pido y saber dÃ³nde buscar recursos es clave.

### ğŸ“š Recursos Clave
* **[Ollama Library](https://ollama.com/library):** Es el "Docker Hub" de los modelos para Ollama. AquÃ­ puedes ver quÃ© modelos existen (Llama 3, Mistral, Phi, Gemma, etc.), cuÃ¡nto ocupan y para quÃ© sirven.
* **[Hugging Face](https://huggingface.co/):** Es el "GitHub" de la Inteligencia Artificial. AquÃ­ es donde la comunidad y las empresas (Meta, Google, Microsoft) publican sus modelos, datasets y demos. Casi todos los modelos de Ollama salen de aquÃ­.
* **[LangChain](https://www.langchain.com/):** (Para curiosos) Es una librerÃ­a (disponible en Java y Python) estÃ¡ndar para crear aplicaciones complejas con IA.

### ğŸ§ª Experimentos (Elige uno)

Para completar la prÃ¡ctica, intenta realizar una de estas pruebas:

1. **Cambio de Modelo:** Busca en la librerÃ­a de Ollama el modelo **`phi`** (de Microsoft) o **`gemma:2b`** (de Google). Entra en tu contenedor, descÃ¡rgalo (`ollama pull nombre_modelo`) y modifica el `index.html` (lÃ­nea 69 aprox) para usar ese modelo en lugar de `tinyllama`. Â¿Responde mejor o peor?
2. **IngenierÃ­a de Prompts:** Intenta "romper" a TinyLlama. Es un modelo pequeÃ±o, asÃ­ que suele alucinar (inventarse cosas) con facilidad. Hazle preguntas de lÃ³gica matemÃ¡tica o sobre hechos histÃ³ricos recientes. Â¿QuÃ© tal se comporta?
3. **AnÃ¡lisis de Red:** Abre las herramientas de desarrollador (F12) -> Red (Network). Manda un mensaje y mira la respuesta "raw" (cruda) que envÃ­a el servidor. Â¿QuÃ© otros datos envÃ­a aparte del texto de la respuesta?

---

## ğŸ“ EntregaPara dar por finalizada la prÃ¡ctica, sube al repositorio:

1. El archivo `docker-compose.yml` que has creado.
2. Una **captura de pantalla** (puedes subirla a la carpeta raiz) donde se vea:
* El chat web funcionando con una respuesta de la IA.![alt text](image-2.png)
  
* Tu terminal de VS Code con el contenedor corriendo (`docker ps`).![alt text](image-1.png)


1. Edita este `README.md` (o crea un archivo `RESPUESTAS.md`) contestando brevemente:
* Â¿QuÃ© modelo has usado finalmente?
  PHI
* Â¿CuÃ¡nto ocupa en disco aproximadamente ese modelo? (BÃºscalo en la web de Ollama).
  2.2G
* Â¿Crees que esta IA responde mÃ¡s rÃ¡pido o mÃ¡s lento que ChatGPT? Â¿Por quÃ© crees que es?
  Yo creo que contesta mas lento porque es una version pequeÃ±a para que me conteste rapido tiene que ser mas grande 
